PROJECT INFO 
This project is about split-learning fine-tuning, a scenario where a neural network that needs to be fine-tuned is split by a communication channel into two sides: a server side and an edge side. 
The goal is to find methods to fine-tune the neural network at the minimum communication cost (the number of information that flows into the channel), while also being dynamically robust to noise.

REPOSITORY STRUCTURE
This repository is built on HYDRA, the configurations are iin the configs folder.

#TODO